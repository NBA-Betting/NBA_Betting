{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NBA AI - Deep Learning Base Model - PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Neural Networks Structure\n",
    "- A neural network comprises various layers: the input layer (for receiving data), hidden layers (where computation and feature extraction occur), and the output layer (producing the final prediction). Each layer contains neurons that perform weighted sums of their inputs followed by an activation function.\n",
    "- **PyTorch Layers**: In PyTorch, `torch.nn.Module` is the base class for all neural network modules. Use `nn.Linear` for fully connected layers, suitable for tabular data. `nn.Conv2d` is used for convolutional layers, which are effective for image data due to their ability to capture spatial hierarchies. `nn.LSTM` or `nn.GRU` layers are used for sequential data like time series or text, capable of capturing temporal dynamics.\n",
    "\n",
    "### 2. Activation Functions\n",
    "- Activation functions introduce non-linear properties to the network. This non-linearity is crucial as it allows the network to learn complex patterns. ReLU (Rectified Linear Unit) is widely used in hidden layers due to its computational efficiency and ability to mitigate the vanishing gradient problem. However, other functions like sigmoid (squashes outputs between 0 and 1) and tanh (outputs between -1 and 1) are also important, particularly in output layers for binary classification or when normalized output is required.\n",
    "- **PyTorch Implementation**: PyTorch provides these activation functions in `torch.nn.functional`. For example, `F.relu` for applying ReLU, `F.sigmoid` for sigmoid, and `F.tanh` for the tanh function. They are typically used within the `forward` method of a `torch.nn.Module` class.\n",
    "\n",
    "### 3. Loss Functions\n",
    "- The choice of the loss function is crucial and should align with the nature of the problem. For regression tasks, Mean Squared Error (MSE) is commonly used as it penalizes larger errors more severely. For classification tasks, Cross-Entropy Loss is standard as it measures the difference between two probability distributions - the actual labels and the predicted probabilities.\n",
    "- **PyTorch Implementation**: PyTorch's `torch.nn` module contains various loss functions. `nn.MSELoss()` is used for regression tasks, and `nn.CrossEntropyLoss()` is used for multi-class classification tasks. CrossEntropyLoss in PyTorch combines a SoftMax activation with the negative log-likelihood loss in one single class.\n",
    "\n",
    "### 4. Optimizers\n",
    "- Optimizers are algorithms used for changing the attributes of the neural network such as weights and learning rate to reduce the losses. Optimizers aim to minimize (or maximize) the loss (or objective) function. While the SGD optimizer is straightforward and effective for many problems, Adam is popular due to its adaptive learning rate capabilities, which can lead to faster convergence.\n",
    "- **PyTorch Implementation**: Optimizers in PyTorch are available under `torch.optim`. For instance, `torch.optim.SGD` for stochastic gradient descent and `torch.optim.Adam` for the Adam optimizer. They require the parameters to optimize (typically obtained using `model.parameters()`) and a learning rate.\n",
    "\n",
    "### 5. Backpropagation and Gradient Descent\n",
    "- Backpropagation is a mechanism used to update the weights of the network efficiently. It calculates the gradient (partial derivatives) of the loss function with respect to each weight in the network by the chain rule, enabling efficient computation of gradients. Gradient Descent, on the other hand, is an optimization algorithm used to minimize the loss function by iteratively moving in the direction of steepest descent as defined by the negative of the gradient.\n",
    "- **PyTorch Mechanism**: In PyTorch, backpropagation is implemented through automatic differentiation provided by the `torch.autograd` module. Use `loss.backward()` to compute the gradient of the loss with respect to each weight and `optimizer.step()` to perform a single optimization step.\n",
    "\n",
    "### 6. Overfitting and Underfitting\n",
    "- Overfitting occurs when a model learns the training data too well, including its noise and outliers, resulting in poor performance on new data. Underfitting occurs when a model is too simple to capture the underlying pattern in the data, leading to poor performance both on the training and new data. Both issues are critical in the development of robust models.\n",
    "- **PyTorch Tools**: PyTorch offers various tools to combat overfitting. For instance, `nn.Dropout` is a layer that randomly zeroes some of the elements of the input tensor with probability `p` during training, which helps prevent overfitting by providing a form of regularization.\n",
    "\n",
    "### 7. Regularization Techniques\n",
    "- Regularization techniques are used to prevent overfitting, which is a common problem in deep learning models. These techniques add information or constraints to the loss function or the network itself to reduce its complexity. Common techniques include L1 and L2 regularization, which add penalties to the loss function based on the size of the weights, and dropout, which randomly drops units from the neural network during training to prevent the network from becoming too dependent on certain pathways.\n",
    "- **PyTorch Implementation**: In PyTorch, L1/L2 regularization is often included in the optimizer's weight decay parameter. Dropout is implemented as a layer (`nn.Dropout`) and can be added to the network architecture in `torch.nn`.\n",
    "\n",
    "### 8. Batch Size and Epochs\n",
    "- The batch size and the number of epochs are hyperparameters that have significant effects on the training process and model performance. The batch size determines how many examples you look at before making a weight update. Smaller batch sizes provide a regularizing effect and lower generalization error. The number of epochs determines how many times the entire dataset is passed forward and backward through the neural network.\n",
    "- **PyTorch Usage**: In PyTorch, the batch size is set when creating a DataLoader object (`torch.utils.data.DataLoader`), which also handles the shuffling and organization of the data. The number of epochs is controlled manually in the training loop.\n",
    "\n",
    "### 9. Learning Rate\n",
    "- The learning rate is a hyperparameter that controls how much to change the model in response to the estimated error each time the model weights are updated. Choosing a learning rate is critical as it makes the model converge too slowly or diverge when too large.\n",
    "- **PyTorch Implementation**: In PyTorch, the learning rate is set when defining an optimizer, e.g., `torch.optim.SGD(model.parameters(), lr=0.01)`. PyTorch also provides learning rate schedulers (e.g., `torch.optim.lr_scheduler`), which adjust the learning rate during training, typically reducing it according to a pre-defined schedule or in response to model performance.\n",
    "\n",
    "### 10. Data Preprocessing\n",
    "- Data preprocessing involves transforming raw data into an understandable format. In deep learning, it often includes normalization (scaling input data to a standard range), and in the case of images, augmentation techniques such as rotations, scaling, and flipping can be used to artificially expand the dataset.\n",
    "- **PyTorch Tools**: For image data, PyTorch offers the `torchvision.transforms` module, which provides common image transformations. For other data types, custom transformations can be applied to the dataset before passing it to a DataLoader.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "* [Data Setup](#data-setup)\n",
    "* [MLP Regression](#mlp-regression)\n",
    "* [MLP Classification](#mlp-classification)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports and Global Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    r2_score,\n",
    "    mean_absolute_error,\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    ")\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# Pandas Settings\n",
    "pd.set_option(\"display.max_columns\", 1000)\n",
    "pd.set_option(\"display.max_rows\", 1000)\n",
    "pd.options.display.max_info_columns = 200\n",
    "pd.options.display.precision = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2021_2022 = pd.read_csv(\"../data/nba_ai/cleaned_data_2021-2022.csv\")\n",
    "df_2022_2023 = pd.read_csv(\"../data/nba_ai/cleaned_data_2022-2023.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"data-setup\"></a>\n",
    "\n",
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_datasets(train_df, cls_target, reg_target, test_df=None, test_size=0.3):\n",
    "    \"\"\"\n",
    "    Prepares datasets for training and testing for both classification and regression targets,\n",
    "    ensuring time-sensitive splitting based on a 'date' column.\n",
    "\n",
    "    Parameters:\n",
    "    train_df (DataFrame): The training dataframe.\n",
    "    cls_target (str): The name of the classification target column.\n",
    "    reg_target (str): The name of the regression target column.\n",
    "    test_df (DataFrame, optional): An optional testing dataframe. If not provided, a portion of the training data is used.\n",
    "    test_size (float, optional): The proportion of the dataset to include in the test split (if test_df is not provided).\n",
    "\n",
    "    Returns:\n",
    "    tuple: A tuple containing six dataframes:\n",
    "           (X_train, X_test, y_train_cls, y_test_cls, y_train_reg, y_test_reg).\n",
    "    \"\"\"\n",
    "\n",
    "    # Sort the dataframe based on the 'date' column\n",
    "    train_df = train_df.sort_values(by=\"date\")\n",
    "\n",
    "    # If a test dataframe is not provided, split the training dataframe\n",
    "    if test_df is None:\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            train_df.drop([cls_target, reg_target], axis=1),\n",
    "            train_df[[cls_target, reg_target]],\n",
    "            test_size=test_size,\n",
    "            shuffle=False,  # Important to maintain time order\n",
    "        )\n",
    "    else:\n",
    "        # If a test dataframe is provided, ensure it is also sorted by date\n",
    "        test_df = test_df.sort_values(by=\"date\")\n",
    "\n",
    "        # Use provided test dataframe and separate features and targets\n",
    "        X_train = train_df.drop([cls_target, reg_target], axis=1)\n",
    "        y_train = train_df[[cls_target, reg_target]]\n",
    "        X_test = test_df.drop([cls_target, reg_target], axis=1)\n",
    "        y_test = test_df[[cls_target, reg_target]]\n",
    "\n",
    "    # Separate classification and regression targets\n",
    "    y_train_cls = y_train[[cls_target]]\n",
    "    y_train_reg = y_train[[reg_target]]\n",
    "    y_test_cls = y_test[[cls_target]]\n",
    "    y_test_reg = y_test[[reg_target]]\n",
    "\n",
    "    return X_train, X_test, y_train_cls, y_test_cls, y_train_reg, y_test_reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train_cls, y_test_cls, y_train_reg, y_test_reg = prepare_datasets(\n",
    "    df_2021_2022, \"CLS_TARGET\", \"REG_TARGET\", test_df=df_2022_2023\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "betting_feature_set = [\n",
    "    \"home_opening_spread\",\n",
    "    \"opening_total\",\n",
    "    \"home_moneyline\",\n",
    "    \"road_moneyline\",\n",
    "]\n",
    "\n",
    "base_feature_set = [\n",
    "    \"day_of_season\",\n",
    "    \"home_team_rest\",\n",
    "    \"road_team_rest\",\n",
    "    \"home_win_pct\",\n",
    "    \"road_win_pct\",\n",
    "    \"home_win_pct_l2w\",\n",
    "    \"road_win_pct_l2w\",\n",
    "    \"home_avg_pts\",\n",
    "    \"road_avg_pts\",\n",
    "    \"home_avg_pts_l2w\",\n",
    "    \"road_avg_pts_l2w\",\n",
    "    \"home_avg_oeff\",\n",
    "    \"road_avg_oeff\",\n",
    "    \"home_avg_oeff_l2w\",\n",
    "    \"road_avg_oeff_l2w\",\n",
    "    \"home_avg_deff\",\n",
    "    \"road_avg_deff\",\n",
    "    \"home_avg_deff_l2w\",\n",
    "    \"road_avg_deff_l2w\",\n",
    "    \"home_avg_eFG%\",\n",
    "    \"road_avg_eFG%\",\n",
    "    \"home_avg_eFG%_l2w\",\n",
    "    \"road_avg_eFG%_l2w\",\n",
    "    \"home_avg_TOV%\",\n",
    "    \"road_avg_TOV%\",\n",
    "    \"home_avg_TOV%_l2w\",\n",
    "    \"road_avg_TOV%_l2w\",\n",
    "    \"home_avg_ORB%\",\n",
    "    \"road_avg_ORB%\",\n",
    "    \"home_avg_ORB%_l2w\",\n",
    "    \"road_avg_ORB%_l2w\",\n",
    "    \"home_avg_FT%\",\n",
    "    \"road_avg_FT%\",\n",
    "    \"home_avg_FT%_l2w\",\n",
    "    \"road_avg_FT%_l2w\",\n",
    "    \"home_avg_pts_allowed\",\n",
    "    \"road_avg_pts_allowed\",\n",
    "    \"home_avg_pts_allowed_l2w\",\n",
    "    \"road_avg_pts_allowed_l2w\",\n",
    "]\n",
    "\n",
    "lineup_vectors = [\"home_lineup_vector\", \"road_lineup_vector\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = base_feature_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_vector_columns(df, vector_columns):\n",
    "    \"\"\"\n",
    "    Flatten vector columns into separate feature columns.\n",
    "\n",
    "    This function takes a DataFrame and a list of column names that store vector data as strings\n",
    "    (typically after being read from a CSV file), and returns a new DataFrame where the vectors\n",
    "    have been flattened into separate feature columns.\n",
    "\n",
    "    Parameters:\n",
    "    df (pandas.DataFrame): The input DataFrame.\n",
    "    vector_columns (list): A list of column names in df that store vector data as strings.\n",
    "\n",
    "    Returns:\n",
    "    pandas.DataFrame: The DataFrame with vector columns flattened.\n",
    "    \"\"\"\n",
    "    for column in vector_columns:\n",
    "        if column not in df.columns:\n",
    "            continue\n",
    "        # Convert the string representation of the vector into a numpy array\n",
    "        df[column] = df[column].apply(\n",
    "            lambda x: np.array(x.strip(\"[]\").replace(\"\\n\", \" \").split(), dtype=float)\n",
    "        )\n",
    "\n",
    "        # Flatten the numpy array into separate columns\n",
    "        vector_df = pd.DataFrame(df[column].tolist(), index=df.index)\n",
    "        vector_df.columns = [f\"{column}_{i}\" for i in range(vector_df.shape[1])]\n",
    "\n",
    "        # Drop the original vector column and concatenate the new DataFrame\n",
    "        df = df.drop(column, axis=1)\n",
    "        df = pd.concat([df, vector_df], axis=1)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train[features]\n",
    "X_test = X_test[features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten lineup vectors\n",
    "X_train = flatten_vector_columns(X_train, lineup_vectors)\n",
    "X_test = flatten_vector_columns(X_test, lineup_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combined Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_train_df = pd.concat([X_train, y_train_cls, y_train_reg], axis=1)\n",
    "combined_test_df = pd.concat([X_test, y_test_cls, y_test_reg], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 1323 entries, 0 to 1322\n",
      "Data columns (total 41 columns):\n",
      " #   Column                    Non-Null Count  Dtype  \n",
      "---  ------                    --------------  -----  \n",
      " 0   day_of_season             1323 non-null   int64  \n",
      " 1   home_team_rest            1323 non-null   int64  \n",
      " 2   road_team_rest            1323 non-null   int64  \n",
      " 3   home_win_pct              1323 non-null   float64\n",
      " 4   road_win_pct              1323 non-null   float64\n",
      " 5   home_win_pct_l2w          1323 non-null   float64\n",
      " 6   road_win_pct_l2w          1323 non-null   float64\n",
      " 7   home_avg_pts              1323 non-null   float64\n",
      " 8   road_avg_pts              1323 non-null   float64\n",
      " 9   home_avg_pts_l2w          1323 non-null   float64\n",
      " 10  road_avg_pts_l2w          1323 non-null   float64\n",
      " 11  home_avg_oeff             1323 non-null   float64\n",
      " 12  road_avg_oeff             1323 non-null   float64\n",
      " 13  home_avg_oeff_l2w         1323 non-null   float64\n",
      " 14  road_avg_oeff_l2w         1323 non-null   float64\n",
      " 15  home_avg_deff             1323 non-null   float64\n",
      " 16  road_avg_deff             1323 non-null   float64\n",
      " 17  home_avg_deff_l2w         1323 non-null   float64\n",
      " 18  road_avg_deff_l2w         1323 non-null   float64\n",
      " 19  home_avg_eFG%             1323 non-null   float64\n",
      " 20  road_avg_eFG%             1323 non-null   float64\n",
      " 21  home_avg_eFG%_l2w         1323 non-null   float64\n",
      " 22  road_avg_eFG%_l2w         1323 non-null   float64\n",
      " 23  home_avg_TOV%             1323 non-null   float64\n",
      " 24  road_avg_TOV%             1323 non-null   float64\n",
      " 25  home_avg_TOV%_l2w         1323 non-null   float64\n",
      " 26  road_avg_TOV%_l2w         1323 non-null   float64\n",
      " 27  home_avg_ORB%             1323 non-null   float64\n",
      " 28  road_avg_ORB%             1323 non-null   float64\n",
      " 29  home_avg_ORB%_l2w         1323 non-null   float64\n",
      " 30  road_avg_ORB%_l2w         1323 non-null   float64\n",
      " 31  home_avg_FT%              1323 non-null   float64\n",
      " 32  road_avg_FT%              1323 non-null   float64\n",
      " 33  home_avg_FT%_l2w          1323 non-null   float64\n",
      " 34  road_avg_FT%_l2w          1323 non-null   float64\n",
      " 35  home_avg_pts_allowed      1323 non-null   float64\n",
      " 36  road_avg_pts_allowed      1323 non-null   float64\n",
      " 37  home_avg_pts_allowed_l2w  1323 non-null   float64\n",
      " 38  road_avg_pts_allowed_l2w  1323 non-null   float64\n",
      " 39  CLS_TARGET                1323 non-null   bool   \n",
      " 40  REG_TARGET                1323 non-null   int64  \n",
      "dtypes: bool(1), float64(36), int64(4)\n",
      "memory usage: 425.1 KB\n"
     ]
    }
   ],
   "source": [
    "combined_train_df.info(verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 1320 entries, 0 to 1319\n",
      "Data columns (total 41 columns):\n",
      " #   Column                    Non-Null Count  Dtype  \n",
      "---  ------                    --------------  -----  \n",
      " 0   day_of_season             1320 non-null   int64  \n",
      " 1   home_team_rest            1320 non-null   int64  \n",
      " 2   road_team_rest            1320 non-null   int64  \n",
      " 3   home_win_pct              1320 non-null   float64\n",
      " 4   road_win_pct              1320 non-null   float64\n",
      " 5   home_win_pct_l2w          1320 non-null   float64\n",
      " 6   road_win_pct_l2w          1320 non-null   float64\n",
      " 7   home_avg_pts              1320 non-null   float64\n",
      " 8   road_avg_pts              1320 non-null   float64\n",
      " 9   home_avg_pts_l2w          1320 non-null   float64\n",
      " 10  road_avg_pts_l2w          1320 non-null   float64\n",
      " 11  home_avg_oeff             1320 non-null   float64\n",
      " 12  road_avg_oeff             1320 non-null   float64\n",
      " 13  home_avg_oeff_l2w         1320 non-null   float64\n",
      " 14  road_avg_oeff_l2w         1320 non-null   float64\n",
      " 15  home_avg_deff             1320 non-null   float64\n",
      " 16  road_avg_deff             1320 non-null   float64\n",
      " 17  home_avg_deff_l2w         1320 non-null   float64\n",
      " 18  road_avg_deff_l2w         1320 non-null   float64\n",
      " 19  home_avg_eFG%             1320 non-null   float64\n",
      " 20  road_avg_eFG%             1320 non-null   float64\n",
      " 21  home_avg_eFG%_l2w         1320 non-null   float64\n",
      " 22  road_avg_eFG%_l2w         1320 non-null   float64\n",
      " 23  home_avg_TOV%             1320 non-null   float64\n",
      " 24  road_avg_TOV%             1320 non-null   float64\n",
      " 25  home_avg_TOV%_l2w         1320 non-null   float64\n",
      " 26  road_avg_TOV%_l2w         1320 non-null   float64\n",
      " 27  home_avg_ORB%             1320 non-null   float64\n",
      " 28  road_avg_ORB%             1320 non-null   float64\n",
      " 29  home_avg_ORB%_l2w         1320 non-null   float64\n",
      " 30  road_avg_ORB%_l2w         1320 non-null   float64\n",
      " 31  home_avg_FT%              1320 non-null   float64\n",
      " 32  road_avg_FT%              1320 non-null   float64\n",
      " 33  home_avg_FT%_l2w          1320 non-null   float64\n",
      " 34  road_avg_FT%_l2w          1320 non-null   float64\n",
      " 35  home_avg_pts_allowed      1320 non-null   float64\n",
      " 36  road_avg_pts_allowed      1320 non-null   float64\n",
      " 37  home_avg_pts_allowed_l2w  1320 non-null   float64\n",
      " 38  road_avg_pts_allowed_l2w  1320 non-null   float64\n",
      " 39  CLS_TARGET                1320 non-null   bool   \n",
      " 40  REG_TARGET                1320 non-null   int64  \n",
      "dtypes: bool(1), float64(36), int64(4)\n",
      "memory usage: 424.1 KB\n"
     ]
    }
   ],
   "source": [
    "combined_test_df.info(verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"mlp-regression\"></a>\n",
    "\n",
    "## Multi-Layer Perceptron (MLP) - Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Pandas DataFrames to PyTorch Tensors\n",
    "X_train_tensor = torch.tensor(X_train.values, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train_reg.values, dtype=torch.float32)\n",
    "\n",
    "# Create a TensorDataset - this wraps tensors into a dataset\n",
    "train_data = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "\n",
    "# DataLoader for batching and shuffling\n",
    "train_loader = DataLoader(train_data, batch_size=32, shuffle=True)\n",
    "\n",
    "# Note: Shuffle is set to True for training data. For time-series data, consider the impact of shuffling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Definition\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a simple regression neural network\n",
    "class RegressionModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(RegressionModel, self).__init__()\n",
    "        self.layer1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.layer2 = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.layer1(x))\n",
    "        x = self.layer2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the model\n",
    "reg_mlp_model = RegressionModel(\n",
    "    input_size=X_train.shape[1], hidden_size=5, output_size=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 74.28823852539062\n",
      "Epoch 10, Loss: 96.0931396484375\n",
      "Epoch 20, Loss: 137.65530395507812\n",
      "Epoch 30, Loss: 195.98065185546875\n",
      "Epoch 40, Loss: 92.8561019897461\n",
      "Epoch 50, Loss: 105.47020721435547\n",
      "Epoch 60, Loss: 335.4338684082031\n",
      "Epoch 70, Loss: 134.19989013671875\n",
      "Epoch 80, Loss: 315.7101135253906\n",
      "Epoch 90, Loss: 275.2518005371094\n"
     ]
    }
   ],
   "source": [
    "# Define loss function and optimizer\n",
    "loss_function = nn.MSELoss()\n",
    "optimizer = optim.Adam(reg_mlp_model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 100  # Set the number of epochs\n",
    "for epoch in range(num_epochs):\n",
    "    for inputs, targets in train_loader:\n",
    "        optimizer.zero_grad()  # Zero the gradient buffers\n",
    "        outputs = reg_mlp_model(inputs)\n",
    "        loss = loss_function(outputs, targets)\n",
    "        loss.backward()  # Backpropagation\n",
    "        optimizer.step()  # Update weights\n",
    "\n",
    "    # Optional: Print the loss every few epochs\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation and Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_tensor = torch.tensor(X_test.values, dtype=torch.float32)\n",
    "\n",
    "# Disable gradient computation for evaluation and prediction\n",
    "with torch.no_grad():\n",
    "    train_predictions_reg = reg_mlp_model(X_train_tensor)\n",
    "    test_predictions_reg = reg_mlp_model(X_test_tensor)\n",
    "\n",
    "# Convert predictions to a NumPy array or Pandas Series for further evaluation\n",
    "train_predictions_reg_np = train_predictions_reg.numpy()\n",
    "test_predictions_reg_np = test_predictions_reg.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mae = mean_absolute_error(train_predictions_reg_np, y_train_reg)\n",
    "train_r2 = r2_score(train_predictions_reg_np, y_train_reg)\n",
    "\n",
    "test_mae = mean_absolute_error(test_predictions_reg_np, y_test_reg)\n",
    "test_r2 = r2_score(test_predictions_reg_np, y_test_reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train MAE: 11.81\n",
      "Train R2: -30.65\n",
      "Test MAE: 10.73\n",
      "Test R2: -38.40\n"
     ]
    }
   ],
   "source": [
    "print(f\"Train MAE: {train_mae:.2f}\")\n",
    "print(f\"Train R2: {train_r2:.2f}\")\n",
    "print(f\"Test MAE: {test_mae:.2f}\")\n",
    "print(f\"Test R2: {test_r2:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Saving and Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Regression_MLP_11.81_10.73_2024-01-03_15-50-13'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "problem_type = \"Regression\"\n",
    "base_model = \"MLP\"\n",
    "train_performance = round(train_mae, 2)\n",
    "test_performance = round(test_mae, 2)\n",
    "\n",
    "model_id = f\"{problem_type}_{base_model}_{train_performance}_{test_performance}_{datetime.datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}\"\n",
    "\n",
    "model_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model state\n",
    "torch.save(reg_mlp_model.state_dict(), f\"../models/{model_id}.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To load the model, first initialize the model structure, then load the state\n",
    "# reg_mlp_model = RegressionModel(input_size=10, hidden_size=5, output_size=1)\n",
    "# reg_mlp_model.load_state_dict(torch.load(f\"../models/{model_id}.pth\"))\n",
    "# reg_mlp_model.eval()  # Set the model to evaluation mode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"mlp-classification\"></a>\n",
    "\n",
    "## Multi-Layer Perceptron (MLP) - Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Pandas DataFrames to PyTorch Tensors\n",
    "X_train_tensor = torch.tensor(X_train.values, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train_cls.values, dtype=torch.float32)\n",
    "\n",
    "# Create a TensorDataset - this wraps tensors into a dataset\n",
    "train_data = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "\n",
    "# DataLoader for batching and shuffling\n",
    "train_loader = DataLoader(train_data, batch_size=32, shuffle=True)\n",
    "\n",
    "# Note: Shuffle is set to True for training data. For time-series data, consider the impact of shuffling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Definition\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassificationModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(ClassificationModel, self).__init__()\n",
    "        self.layer1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.layer2 = nn.Linear(\n",
    "            hidden_size, 1\n",
    "        )  # Output size is 1 for binary classification\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.layer1(x))\n",
    "        x = torch.sigmoid(\n",
    "            self.layer2(x)\n",
    "        )  # Sigmoid activation for binary classification\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the model\n",
    "cls_mlp_model = ClassificationModel(input_size=X_train.shape[1], hidden_size=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 0.7315127849578857\n",
      "Epoch 10, Loss: 0.6890249848365784\n",
      "Epoch 20, Loss: 0.690775990486145\n",
      "Epoch 30, Loss: 0.6824485063552856\n",
      "Epoch 40, Loss: 0.704181969165802\n",
      "Epoch 50, Loss: 0.6825932860374451\n",
      "Epoch 60, Loss: 0.7145680785179138\n",
      "Epoch 70, Loss: 0.6818594336509705\n",
      "Epoch 80, Loss: 0.6659519672393799\n",
      "Epoch 90, Loss: 0.6734258532524109\n"
     ]
    }
   ],
   "source": [
    "# Define loss function and optimizer for binary classification\n",
    "loss_function = nn.BCELoss()\n",
    "optimizer = optim.Adam(cls_mlp_model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    for inputs, targets in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = cls_mlp_model(inputs)\n",
    "        loss = loss_function(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation and Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_tensor = torch.tensor(X_test.values, dtype=torch.float32)\n",
    "\n",
    "# Disable gradient computation for evaluation and prediction\n",
    "with torch.no_grad():\n",
    "    train_predictions_cls = cls_mlp_model(X_train_tensor)\n",
    "    test_predictions_cls = cls_mlp_model(X_test_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming your model outputs probabilities for class 1\n",
    "threshold = 0.5\n",
    "\n",
    "# Convert probabilities to class labels based on the threshold\n",
    "train_predictions_cls_np = train_predictions_cls.numpy() > threshold\n",
    "test_predictions_cls_np = test_predictions_cls.numpy() > threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_accuracy = accuracy_score(train_predictions_cls_np, y_train_cls)\n",
    "train_precision = precision_score(train_predictions_cls_np, y_train_cls)\n",
    "\n",
    "test_accuracy = accuracy_score(test_predictions_cls_np, y_test_cls)\n",
    "test_precision = precision_score(test_predictions_cls_np, y_test_cls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.52\n",
      "Train Precision: 0.00\n",
      "Test Accuracy: 0.48\n",
      "Test Precision: 0.00\n"
     ]
    }
   ],
   "source": [
    "print(f\"Train Accuracy: {train_accuracy:.2f}\")\n",
    "print(f\"Train Precision: {train_precision:.2f}\")\n",
    "print(f\"Test Accuracy: {test_accuracy:.2f}\")\n",
    "print(f\"Test Precision: {test_precision:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Saving and Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Classification_MLP_11.81_10.73_2024-01-03_15-50-20'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "problem_type = \"Classification\"\n",
    "base_model = \"MLP\"\n",
    "train_performance = round(train_mae, 2)\n",
    "test_performance = round(test_mae, 2)\n",
    "\n",
    "model_id = f\"{problem_type}_{base_model}_{train_performance}_{test_performance}_{datetime.datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}\"\n",
    "\n",
    "model_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model state\n",
    "torch.save(cls_mlp_model.state_dict(), f\"../models/{model_id}.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To load the model, first initialize the model structure, then load the state\n",
    "# cls_mlp_model = ClassificationModel(input_size=10, hidden_size=5, output_size=1)\n",
    "# cls_mlp_model.load_state_dict(torch.load(f\"../models/{model_id}.pth\"))\n",
    "# cls_mlp_model.eval()  # Set the model to evaluation mode"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nba_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
